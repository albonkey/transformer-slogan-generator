{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, DataCollatorForSeq2Seq, Trainer, TrainingArguments\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(\"kaggle.csv\")\n",
    "df.dropna(subset=['company', 'description', 'slogan'], inplace=True)\n",
    "\n",
    "# Prepare the dataset by adding a combined text column and labels\n",
    "df['text'] = df['company'] + \" described as: \" + df['description']\n",
    "df['labels'] = df['slogan']\n",
    "\n",
    "# Split data into training and evaluation sets\n",
    "train_df, eval_df = train_test_split(df, test_size=0.1)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Dataset.from_pandas(train_df[['text', 'labels']])\n",
    "eval_dataset = Dataset.from_pandas(eval_df[['text', 'labels']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480a48f9686940099d164074ec047e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4509 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5635f2ab162469f94b69d09f7f0b154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/501 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize tokenizer and model\n",
    "model_name = \"facebook/bart-large\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Define tokenization and encoding function for datasets\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(examples['text'], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    # Tokenize the labels as well\n",
    "    labels = tokenizer(examples['labels'], max_length=512, truncation=True, padding=\"max_length\").input_ids\n",
    "    model_inputs['labels'] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# Apply function to datasets\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d902bf0931774dfd96cfe4b6f8dc3934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1692 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:587: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 15.1772, 'grad_norm': 15.295199394226074, 'learning_rate': 4.5e-06, 'epoch': 0.09}\n",
      "{'loss': 9.6389, 'grad_norm': 76.83898162841797, 'learning_rate': 9.5e-06, 'epoch': 0.18}\n",
      "{'loss': 6.1698, 'grad_norm': 40.09737014770508, 'learning_rate': 1.45e-05, 'epoch': 0.27}\n",
      "{'loss': 4.3322, 'grad_norm': 42.46813201904297, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.35}\n",
      "{'loss': 1.8672, 'grad_norm': 21.56826400756836, 'learning_rate': 2.45e-05, 'epoch': 0.44}\n",
      "{'loss': 0.2469, 'grad_norm': 1.3921730518341064, 'learning_rate': 2.95e-05, 'epoch': 0.53}\n",
      "{'loss': 0.108, 'grad_norm': 0.9650602340698242, 'learning_rate': 3.45e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0817, 'grad_norm': 0.5644066333770752, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0749, 'grad_norm': 0.4798884093761444, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0756, 'grad_norm': 0.7262111306190491, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.89}\n",
      "{'loss': 0.0713, 'grad_norm': 0.9041557312011719, 'learning_rate': 4.8112416107382555e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba064a3ef3f74bd3bf53d0f24e36fc03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06451698392629623, 'eval_runtime': 99.8792, 'eval_samples_per_second': 5.016, 'eval_steps_per_second': 0.631, 'epoch': 1.0}\n",
      "{'loss': 0.0666, 'grad_norm': 0.4116945266723633, 'learning_rate': 4.6015100671140944e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0615, 'grad_norm': 0.3676556348800659, 'learning_rate': 4.391778523489933e-05, 'epoch': 1.15}\n",
      "{'loss': 0.062, 'grad_norm': 0.4194311499595642, 'learning_rate': 4.1820469798657716e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0612, 'grad_norm': 0.3710213303565979, 'learning_rate': 3.972315436241611e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0631, 'grad_norm': 0.407609760761261, 'learning_rate': 3.76258389261745e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0604, 'grad_norm': 0.37391895055770874, 'learning_rate': 3.552852348993288e-05, 'epoch': 1.51}\n",
      "{'loss': 0.0616, 'grad_norm': 0.7479476928710938, 'learning_rate': 3.343120805369128e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0591, 'grad_norm': 0.3569755554199219, 'learning_rate': 3.133389261744967e-05, 'epoch': 1.68}\n",
      "{'loss': 0.0641, 'grad_norm': 0.47439122200012207, 'learning_rate': 2.9236577181208053e-05, 'epoch': 1.77}\n",
      "{'loss': 0.0592, 'grad_norm': 0.3955216109752655, 'learning_rate': 2.7139261744966442e-05, 'epoch': 1.86}\n",
      "{'loss': 0.058, 'grad_norm': 0.6031571626663208, 'learning_rate': 2.5041946308724835e-05, 'epoch': 1.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd98412182b4cca89fb217b913f4556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.060410358011722565, 'eval_runtime': 99.8441, 'eval_samples_per_second': 5.018, 'eval_steps_per_second': 0.631, 'epoch': 2.0}\n",
      "{'loss': 0.0485, 'grad_norm': 0.4170510470867157, 'learning_rate': 2.2944630872483224e-05, 'epoch': 2.04}\n",
      "{'loss': 0.0439, 'grad_norm': 0.6888560652732849, 'learning_rate': 2.0847315436241613e-05, 'epoch': 2.13}\n",
      "{'loss': 0.0447, 'grad_norm': 0.4819971024990082, 'learning_rate': 1.8750000000000002e-05, 'epoch': 2.22}\n",
      "{'loss': 0.045, 'grad_norm': 0.48874902725219727, 'learning_rate': 1.6652684563758387e-05, 'epoch': 2.3}\n",
      "{'loss': 0.0454, 'grad_norm': 0.28788232803344727, 'learning_rate': 1.455536912751678e-05, 'epoch': 2.39}\n",
      "{'loss': 0.0444, 'grad_norm': 0.2739470899105072, 'learning_rate': 1.2458053691275167e-05, 'epoch': 2.48}\n",
      "{'loss': 0.0443, 'grad_norm': 0.3478606939315796, 'learning_rate': 1.0360738255033558e-05, 'epoch': 2.57}\n",
      "{'loss': 0.0568, 'grad_norm': 0.2987494468688965, 'learning_rate': 8.263422818791947e-06, 'epoch': 2.66}\n",
      "{'loss': 0.0442, 'grad_norm': 0.4316296875476837, 'learning_rate': 6.166107382550336e-06, 'epoch': 2.75}\n",
      "{'loss': 0.044, 'grad_norm': 0.5976526141166687, 'learning_rate': 4.068791946308725e-06, 'epoch': 2.84}\n",
      "{'loss': 0.0422, 'grad_norm': 0.3486897051334381, 'learning_rate': 1.971476510067114e-06, 'epoch': 2.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592b4bcdc51e4db5bc9395e96fa961e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06017742678523064, 'eval_runtime': 109.1397, 'eval_samples_per_second': 4.59, 'eval_steps_per_second': 0.577, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 19500.1913, 'train_samples_per_second': 0.694, 'train_steps_per_second': 0.087, 'train_loss': 1.1543080033844526, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./final_bart_model\\\\tokenizer_config.json',\n",
       " './final_bart_model\\\\special_tokens_map.json',\n",
       " './final_bart_model\\\\vocab.json',\n",
       " './final_bart_model\\\\merges.txt',\n",
       " './final_bart_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./bartresults',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    warmup_steps=500,\n",
    "    prediction_loss_only=True,\n",
    "    fp16=True  # Assuming CUDA and compatible GPU\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained('./final_bart_model')\n",
    "tokenizer.save_pretrained('./final_bart_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Slogan: SkyCompute Technologies | AI enabled cloud solutions for your business\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Function to load the model and tokenizer\n",
    "def load_model(model_path):\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Function to generate a slogan\n",
    "def generate_slogan(model, tokenizer, company_name, description, device='cuda'):\n",
    "    # Combine company name and description into a single input text\n",
    "    input_text = f\"{company_name} described as: {description}\"\n",
    "    \n",
    "    # Encode the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Generate output ids, correctly using the temperature parameter\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=80,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,  # Ensures some creativity\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    \n",
    "    # Decode the output ids to a string\n",
    "    slogan = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return slogan\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to your trained model\n",
    "    model_path = './final_bart_model'\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    model, tokenizer = load_model(model_path)\n",
    "    \n",
    "    # Ensure the model is on the appropriate device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    \n",
    "    # Input your own company name and description\n",
    "    company_name = \"SkyCompute Technologies\"\n",
    "    description = \"SkyCompute Technologies pioneers advanced cloud solutions to streamline business operations and enhance data management. Our flagship product, the 'SkyNode', integrates AI capabilities seamlessly into your infrastructure, supported by our commitment to security, performance, and eco-friendly practices. We empower organizations to innovate and scale efficiently, ensuring a robust digital transformation journey.\"\n",
    "    \n",
    "    # Generate slogan\n",
    "    slogan = generate_slogan(model, tokenizer, company_name, description, device)\n",
    "    print(\"Generated Slogan:\", slogan)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
